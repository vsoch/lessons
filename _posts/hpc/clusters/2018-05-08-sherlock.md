---
date: 2018-05-07
title: "Sherlock Cluster"
description: Getting started with the Sherlock Cluster at Stanford University
categories:
  - transfer
resources:
  - name: "Sherlock 2 Getting Started"
    link: https://www.sherlock.stanford.edu/docs/getting-started/
  - name: Sherlock (Deprecated) Documentation
    link: https://sherlock.stanford.edu
  - name: SLURM Commands
    link: http://slurm.schedmd.com/pdfs/summary.pdf
  - name: Email Research Computing
    link: "mailto:research-computing-support@stanford.edu"
type: Document
set: clusters
set_order: 2
tags: [resources]
---


Small to medium scale jobs can be done on Sherlock, a local computing cluster at Stanford. This will serve as a "quick start" guide, as well as documentation for running jobs. To see the full documentation base, <a href="https://www.sherlock.stanford.edu/docs/getting-started/" target="_blank">see here</a>.

## What is Sherlock?
Sherlock is a large compute cluster at Stanford University.  This means that you can 
login to Sherlock from a secure shell (SSH), and run scripts across many computational nodes. 
After you have <a href="https://www.sherlock.stanford.edu/docs/getting-started/#how-to-request-an-account" target="_blank">
requested an acconut</a>, this quick start guide should get your familiar with working with Sherlock.

### 1. Connect to Sherlock
Sherlock uses <a href="{{ site.baseurl }}/kerberos">Kerberos Authentication</a>, and
you can read more and find instructions for setting it up by following the link. Basically, once you have this setup,
you will be able to authenticate and ssh to sherlock as follows:

```bash
$ kinit yourSUnet@stanford.edu

# see your authentications and check when your identification expires
$ klist 
$ ssh â€“XY yourSUnet@sherlock.stanford.edu
```

When your login to Sherlock doesn't work, you likely just need to re-initialize your kinit:

```bash
kinit myemail@stanford.edu
```

because your ticket is expired.

### 2. What can you do?

In a nutshell, Sherlock gives you access to a job manager to run jobs at scale (SLURM),
an ability to run programs that require graphics or display (with X11), data transfer options,
and installation of custom software. Summary of the above functions will be shown here, and 
more detailed tutorials will be provided.


### 3. Important Locations
Okay, here is the organization of sherlock in a nutshell. The places that you hit when you login
are the `login nodes`. They are literally a set of machines that exist just for this purpose.
From the login nodes, you can submit jobs via the `SLURM job manager`. The submission is handled
by a master node queue, which sends out jobs to the workers, which are other machines on the cluster.

```bash
                +------------+
                |            |
                |  Worker 2  |
+------------+  |  (GPU)     |
|            |  |            |
|  Worker 1  |  +------------+
|            |
+----^-------+   +----------------------+
     |           | Priority Queue       |
     +-----------+ 1. Job A UserMark    |
     2. Run Job  | 2. Job B UserFred    |
                 |                      |
                 +-^--------------------+
                   |
                   |
                   |
                   |  1. Submit Job (qsub...)
                   |
     +-------------+-+  +---------------+
     |               |  |               |
     |               |  |               |
     |  Login Node 1 |  |  Login Node 2 |
     |               |  |               |
     |               |  |               |
     +---------------+  +---------------+
```

In the diagram above, you hit one of the two login nodes from your local machine. You
then issue commands to interact with the SLURM priority queue (center node). It's this master
that then submits jobs to the various worker nodes on the cluster. Both the login nodes
and the worker nodes have access to two important places - various "homes" that are backed up
where you can store files and code, and "scratch" or working spaces for larger datasets.

 - *$HOME* is your home folder, the entrypoint at login. It's a high speed NFS filesystem (Isilon) and is backed up for you. You can edit the `.bash_profile` here to add programs and other environment variables to your path, or have modules for software you use often loaded automatically (we will write another post on modules). You should be very careful about putting large content (whether data or programs) in your $HOME, because the space can fill up quickly.
 - *SCRATCH* is a larger storage space where you are encouraged to put data (and larger) files. This could include caching of container images (e.g., Singularity) or other intermediate data files.
 - *$PI_SCRATCH* is a scratch space allocated to a PI, which usually is best used for shared data in a lab.

For quotas, technical specifications, and more detail on these systems, see 
<a href="https://www.sherlock.stanford.edu/docs/overview/specs/" target="_blank">this page</a>.
 

## Filesystems

### Getting Comfy In Home

When you first login, you are in your home folder.  If you type `ls` there will be nothing there.  This is where you should create a nice organized file structure for your scripts, and any software that you install locally.  If you type

```bash
$ echo $HOME
```

you will see the path with your SUnet id. At any point in time you can get your present working directory (pwd) if you type:

```bash
$ echo $PWD

# or
$ pwd
```

If you do:

```bash
$ echo $PI_HOME
```

you will see that you also have a shared group space under your PI's name. 
The quota is slightly larger here, and it would be good for shared group files and software.


### Getting Comfy with Scratch

 - We have both `$SCRATCH` and `$PI_SCRATCH`, and the latter is shared with your group.
 - `$LOCAL_SCRATCH`: is used for storage of temporary files. These files are not backed up, and are deleted when a job finishes running.
 - It's recommended to create a good organizational hierarchy for your personal and group data before putting it there.
 - It's also recommended to discuss a data storage and archival strategy, which will be best appreciated down the road when you lab amasses a lot of it.

**How do I transfer files here?**
In short, you can use secure copy (the `scp` command), <a href="https://www.globus.org" target="_blank">Globus Online</a>, a secure shell client, or our data transfer note. We have prepared a nice walkthrough of these options <a href="https://www.sherlock.stanford.edu/docs/user-guide/storage/data-transfer/#transfer-protocols" target="_blank">here</a>.


### Quotas

Everything in `$LOCAL_SCRATH` is deleted after the job is run. `$SCRATCH` is purged when necessary. If your lab has storage on `$OAK`, this is 
where you can reliably store data and it won't be purged.

**How do I check disk quota**

```bash
$ lfs quota -u <sunetid> -h /scratch/ # or
$ lfs quota -g <pi_sunetid> -h /scratch/
```

**How do I count files?*
```bash
find . -type f | wc -l
```

**How is the quota calculated?**

Keep your space tidy! The data that are stored in your personal `$SCRATCH` or `$HOME` folders can 
also contribute to the quota in `$PI_SCRATCH` and `$PI_HOME`. For example:

 - Files that belong to you as a user (`$USER`) count towards your user quota
 - Files that belong to your group count towards your group quota
 - Files that belong to `$USER`:group count towards both quotas, user and group

If you belong to multiple groups, you can change the ownership of the files to any of the groups, and
the quota attribution will change along with it.


## Command Quick Reference

**Jobs**

```bash
# Submit a job
sbatch .jobs/jobFile.job

# See the entire job queue (note that you are allowed 5000 in queue at once)
squeue

# See only jobs for a given user
squeue -u username

# Crap, kill a job with ID $PID
scancel $PID

# Kill ALL jobs for a user
scancel -u username

# Kill all pending jobs
scancel -u username --state=pending

# Stop and restart jobs interactively with SLURM's scancel.
scancel -s SIGSTOP job id

# Run interactive node with 16 cores (12 plus all memory on 1 node) for 4 hours:
srun -n 12 -N 1 --mem=64000 --time 4:0:0 --pty bash

# Claim interactive node for exclusive use, 8 hours
srun --exclusive --time 8:0:0 --pty bash

# Same as above, but with X11 forwarding
srun --exclusive --time 8:0:0 --x11 --pty bash

# Same as above, but with priority over your other jobs
srun --nice=9999 --exclusive --time 8:0:0 --x11 --pty -p dev -t 12:00 bash

# Count number of running / in queue jobs
squeue -u username | wc -l

# Get estimated start times for your jobs (when Sherlock is busy)
squeue --start -u username

# Request big memory node for 4 hours
srun -N 1 -p bigmem -t 4:0:0 --pty bash

# Run a job with 1 node, 4 CPU cores, and 2 GPU
-N 1 -n 4 --gres=gpy:2 -p gpu --qos=gpu
```

**Big Memory Nodes**

```bash
# Submit a job
sbatch -p bigmem --qos bigmem /path/to/job/gica.job

# Interactive job
srun -p bigmem --qos bigmem --time 1:00 --exclusive --x11 --pty bash
```

**Software Modules**

```bash
# What modules are available?
module avail

# Load a module
module load MODULE_NAME

# Some modules need to be loaded under a group first
module load system
module load singularity/2.4.5
```

**Disk Usage**

```bash
# Get usage for file systems
df -k

# Get usage for your home directory (useful if you are close to quota)
du
```

### Interactive Jobs

```bash
srun -t 2-00 --pty bash
```

With this command you get an interactive node available for 2 days (-t 2-00) in an interactive mode (--pty) 
bash shell. If you want to be the exclusive user of the node ("I want all the memory!" (diabolical laugh) add `--exclusive`.

You can also designate how much memory you would like to use:

```
srun --mem=32G --pty bash
```

## Jobs

**How do I run jobs?**
This is the most important function that you must do! See [this guide]() for a detailed introduction
and walk through. There are two kinds of jobs:

 - **interactive jobs**: means jumping yourself onto a worker node, and issuing commands interactively. If you want to test or otherwise run commands manually, you should ask for an interactive node. You should **not be running things on the login node** because it hinders performance for other users. Your processes will be killed if they take up too much memory.
 - **batch jobs** means handing a submission script to the SLURM batch scheduler to give instruction for running one or more jobs. We provide detailed instructions in the link above, and quick command references below.


### Batch Jobs
Let's say that we have a script that we want to run, `analysis.sh`. We can hand the
script off to be run on a node as follows:

```bash
sbatch  -o out.%j -e err.%j analysis.sh arg1 value1
```

(Standard) Output of this script will be saved as `out.%j` (`%j` will be replaced with the jobID) 
in the current directory, and `$STDERR` will be in `err.%j`.
The `.out` and `.job` files can be written where you will easily find them, and this is a good
strategy for debugging. I usually like to keep an `.out`, and `.job` folder in my script directory
so they are organized and easy to find. The string `%j` will produce a unique identifier for the job, and the
`arg1` and `value1` are example input args and values for your script.

An example job script is shown below.

```bash
#!/bin/bash
#SBATCH --job-name=TRM000_phonetic.job
#SBATCH --output=TRM000_phonetic.job.out
#SBATCH --error=TRM000_phonetic.job.err
#SBATCH --time=2-00:00
#SBATCH --mem=12000
#SBATCH --qos=normal
#SBATCH --mail-type=ALL
#SBATCH --mail-user=$USER@stanford.edu
Rscript $HOME/shapleyProbeSets.R $SCRATCH/DATA/input.tab 0.05 1000 myJob.R
```

The above job will submit a job named `TRM000_phonetic.job`, and write to correponding error and output files. It will
have a maximum run time of 2 days, ask for 12GB memory, and be submit to the normal queue.


### Job Status

- `sinfo`: show the overall status of each partition, like how many nodes are idle per partition.
- `squeue`: display all jobs currently running, per given condition.
- `sstat -j jobID`: show the status of a currently running job.
- `sacct -j jobID`: show the final status of a finished job.
- `scancel`: cancel job(s). You can specify job state/conditions to cancel (e.g., `--state PENDING`).

Show jobs from userID
```bash
squeue -u userID
```

We will be writing a more detailed job guide, stay tuned!

## Software
Software comes by way of system installed software (modules) and software that you can bring
(containers) or install in your user home. We will have a separate, detailed page on Software.
